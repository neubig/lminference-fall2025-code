{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "942cc043",
   "metadata": {},
   "source": [
    "# Qwen Model Completion Analysis\n",
    "\n",
    "This notebook demonstrates text generation using the Qwen 3-1.7B-Base model and analyzes\n",
    "the probability distributions of generated completions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll explore:\n",
    "- Unconditional text generation (from BOS token)\n",
    "- Conditional text generation (from a specific prompt)\n",
    "- Analysis of completion probabilities\n",
    "- Comparison of high, low, and medium probability completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a619bae8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress some warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b49fb2",
   "metadata": {},
   "source": [
    "## Model Setup and Configuration\n",
    "\n",
    "First, let's set up the Qwen model and tokenizer. We'll handle device selection automatically\n",
    "and include robust error handling for different hardware configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca84a6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class QwenCompletionGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating text completions using Qwen models with probability analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"Qwen/Qwen3-1.7B-Base\"):\n",
    "        \"\"\"Initialize the Qwen model and tokenizer.\"\"\"\n",
    "        # Determine the best available device\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda:0\")\n",
    "            logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            logger.info(\"Using CPU\")\n",
    "        \n",
    "        logger.info(f\"Loading model: {model_name}\")\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        # Load model with appropriate settings\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,\n",
    "            device_map=None  # Manual device placement for better control\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Set pad token if not exists\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        logger.info(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Display model info\n",
    "        num_params = sum(p.numel() for p in self.model.parameters())\n",
    "        logger.info(f\"Model parameters: {num_params:,} ({num_params/1e6:.1f}M)\")\n",
    "\n",
    "# Initialize the model\n",
    "generator = QwenCompletionGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a40be",
   "metadata": {},
   "source": [
    "## Text Generation with Probability Tracking\n",
    "\n",
    "Now let's implement the core generation function that tracks token probabilities\n",
    "and stops at natural sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948add31",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_completion(self, prompt: str, max_length: int = 50) -> Tuple[str, float]:\n",
    "    \"\"\"\n",
    "    Generate a completion and compute its total log-probability.\n",
    "    Stops at first period or newline for natural sentence boundaries.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt (empty string for unconditional generation)\n",
    "        max_length: Maximum number of tokens to generate\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (generated_text, total_log_probability)\n",
    "    \"\"\"\n",
    "    # Handle empty prompt case - use a space for unconditional generation\n",
    "    if not prompt.strip():\n",
    "        prompt = \" \"\n",
    "    \n",
    "    # Tokenize the prompt\n",
    "    inputs = self.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True).to(self.device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    eos_id = self.tokenizer.eos_token_id\n",
    "\n",
    "    # Generate with sampling and probability tracking\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=1.0,\n",
    "                pad_token_id=eos_id,\n",
    "                eos_token_id=eos_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                top_k=50,\n",
    "                top_p=0.95\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Generation failed: {e}. Trying with fallback parameters...\")\n",
    "            # Fallback with more conservative parameters\n",
    "            output = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                attention_mask=inputs.attention_mask,\n",
    "                max_new_tokens=max_length,\n",
    "                do_sample=True,\n",
    "                temperature=0.8,\n",
    "                pad_token_id=eos_id,\n",
    "                eos_token_id=eos_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                top_k=40,\n",
    "                top_p=0.9\n",
    "            )\n",
    "\n",
    "        # Extract generated tokens (excluding input)\n",
    "        gen_ids = output.sequences[0][input_length:]\n",
    "\n",
    "        total_log_prob = 0.0\n",
    "        tokens_used = 0\n",
    "        displayed_text = \"\"\n",
    "\n",
    "        # Calculate probabilities and find stopping point\n",
    "        for i, (logits, tok_id) in enumerate(zip(output.scores, gen_ids)):\n",
    "            # Calculate log probabilities\n",
    "            log_probs = torch.log_softmax(logits[0], dim=-1)\n",
    "            total_log_prob += log_probs[tok_id.item()].item()\n",
    "            tokens_used = i + 1\n",
    "\n",
    "            # Decode incrementally to detect stopping conditions\n",
    "            partial_text = self.tokenizer.decode(gen_ids[:tokens_used], skip_special_tokens=True)\n",
    "            if ('.' in partial_text or '\\n' in partial_text or \n",
    "                (eos_id is not None and tok_id.item() == eos_id)):\n",
    "                displayed_text = partial_text\n",
    "                break\n",
    "\n",
    "        if not displayed_text:\n",
    "            displayed_text = self.tokenizer.decode(gen_ids[:tokens_used], skip_special_tokens=True)\n",
    "\n",
    "        # Clean up the text\n",
    "        if '.' in displayed_text:\n",
    "            displayed_text = displayed_text.split('.')[0] + '.'\n",
    "        elif '\\n' in displayed_text:\n",
    "            displayed_text = displayed_text.split('\\n')[0]\n",
    "        else:\n",
    "            displayed_text = displayed_text.strip()\n",
    "\n",
    "    return displayed_text, total_log_prob\n",
    "\n",
    "# Add the method to our generator instance\n",
    "QwenCompletionGenerator.generate_completion = generate_completion\n",
    "\n",
    "# Test the generation function\n",
    "test_completion, test_prob = generator.generate_completion(\"The weather today is\")\n",
    "print(f\"Test completion: '{test_completion}' (log_prob: {test_prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdfec0",
   "metadata": {},
   "source": [
    "## Batch Generation and Analysis\n",
    "\n",
    "Let's implement functions to generate multiple completions and analyze their probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59515cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiple_completions(self, prompt: str, num_completions: int = 100) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Generate multiple completions from the given prompt.\"\"\"\n",
    "    completions = []\n",
    "    \n",
    "    logger.info(f\"Generating {num_completions} completions for prompt: '{prompt[:50]}...'\")\n",
    "    \n",
    "    for i in range(num_completions):\n",
    "        if (i + 1) % 20 == 0:\n",
    "            logger.info(f\"Progress: {i + 1}/{num_completions} completions\")\n",
    "        \n",
    "        completion, log_prob = self.generate_completion(prompt)\n",
    "        completions.append((completion, log_prob))\n",
    "    \n",
    "    return completions\n",
    "\n",
    "def select_completions_by_probability(self, completions: List[Tuple[str, float]]) -> Tuple[List[Tuple[str, float]], List[Tuple[str, float]], List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    Select highest, lowest, and central probability completions.\n",
    "    \"\"\"\n",
    "    # Sort by log probability (higher is better)\n",
    "    sorted_completions = sorted(completions, key=lambda x: x[1], reverse=True)\n",
    "    n = len(sorted_completions)\n",
    "    \n",
    "    # Determine selection size based on total completions\n",
    "    select_size = 5 if n >= 100 else min(3, n // 3) if n >= 9 else min(2, n // 2) if n >= 6 else 1\n",
    "    \n",
    "    # Get highest probability completions\n",
    "    highest = sorted_completions[:select_size]\n",
    "    \n",
    "    # Get lowest probability completions\n",
    "    lowest = sorted_completions[-select_size:]\n",
    "    \n",
    "    # Get central probability completions (around median)\n",
    "    if n >= 3:\n",
    "        mid_start = max(0, (n - select_size) // 2)\n",
    "        central = sorted_completions[mid_start:mid_start + select_size]\n",
    "    else:\n",
    "        central = sorted_completions[:1]\n",
    "    \n",
    "    return highest, lowest, central\n",
    "\n",
    "# Add methods to our generator\n",
    "QwenCompletionGenerator.generate_multiple_completions = generate_multiple_completions\n",
    "QwenCompletionGenerator.select_completions_by_probability = select_completions_by_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8088e98a",
   "metadata": {},
   "source": [
    "## Experiment 1: Unconditional Generation (BOS Token)\n",
    "\n",
    "Let's start by generating completions from the beginning-of-sequence token to see\n",
    "what the model generates unconditionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 1: Unconditional Generation from BOS Token\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate completions from BOS token (empty prompt)\n",
    "bos_prompt = \"\"\n",
    "bos_completions = generator.generate_multiple_completions(bos_prompt, 100)\n",
    "bos_highest, bos_lowest, bos_central = generator.select_completions_by_probability(bos_completions)\n",
    "\n",
    "print(f\"\\nGenerated {len(bos_completions)} completions from BOS token\")\n",
    "print(f\"Unique completions: {len(set(comp[0] for comp in bos_completions))}\")\n",
    "\n",
    "print(\"\\nüî• HIGHEST Probability Completions:\")\n",
    "for i, (text, prob) in enumerate(bos_highest, 1):\n",
    "    print(f\"{i:2d}. (log_prob: {prob:7.4f}) '{text}'\")\n",
    "\n",
    "print(\"\\n‚ùÑÔ∏è  LOWEST Probability Completions:\")\n",
    "for i, (text, prob) in enumerate(bos_lowest, 1):\n",
    "    print(f\"{i:2d}. (log_prob: {prob:7.4f}) '{text}'\")\n",
    "\n",
    "print(\"\\nüìä MEDIAN Probability Completions:\")\n",
    "for i, (text, prob) in enumerate(bos_central, 1):\n",
    "    print(f\"{i:2d}. (log_prob: {prob:7.4f}) '{text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4bac7a",
   "metadata": {},
   "source": [
    "## Experiment 2: Conditional Generation (CMU Prompt)\n",
    "\n",
    "Now let's generate completions conditioned on a specific prompt about Carnegie Mellon University."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612690f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT 2: Conditional Generation from CMU Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cmu_prompt = \"The best thing about Carnegie Mellon University is\"\n",
    "cmu_completions = generator.generate_multiple_completions(cmu_prompt, 100)\n",
    "cmu_highest, cmu_lowest, cmu_central = generator.select_completions_by_probability(cmu_completions)\n",
    "\n",
    "print(f\"\\nGenerated {len(cmu_completions)} completions for prompt: '{cmu_prompt}'\")\n",
    "print(f\"Unique completions: {len(set(comp[0] for comp in cmu_completions))}\")\n",
    "\n",
    "print(\"\\nüî• HIGHEST Probability Completions:\")\n",
    "for i, (text, prob) in enumerate(cmu_highest, 1):\n",
    "    print(f\"{i:2d}. (log_prob: {prob:7.4f}) '{cmu_prompt} {text}'\")\n",
    "\n",
    "print(\"\\n‚ùÑÔ∏è  LOWEST Probability Completions:\")\n",
    "for i, (text, prob) in enumerate(cmu_lowest, 1):\n",
    "    print(f\"{i:2d}. (log_prob: {prob:7.4f}) '{cmu_prompt} {text}'\")\n",
    "\n",
    "print(\"\\nüìä MEDIAN Probability Completions:\")\n",
    "for i, (text, prob) in enumerate(cmu_central, 1):\n",
    "    print(f\"{i:2d}. (log_prob: {prob:7.4f}) '{cmu_prompt} {text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee48790b",
   "metadata": {},
   "source": [
    "## Statistical Analysis and Comparison\n",
    "\n",
    "Let's analyze the statistical properties of the generated completions and compare\n",
    "the two experimental conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract probabilities for analysis\n",
    "bos_probs = [prob for _, prob in bos_completions]\n",
    "cmu_probs = [prob for _, prob in cmu_completions]\n",
    "\n",
    "# Calculate statistics\n",
    "bos_stats = {\n",
    "    'mean': np.mean(bos_probs),\n",
    "    'std': np.std(bos_probs),\n",
    "    'min': np.min(bos_probs),\n",
    "    'max': np.max(bos_probs),\n",
    "    'median': np.median(bos_probs)\n",
    "}\n",
    "\n",
    "cmu_stats = {\n",
    "    'mean': np.mean(cmu_probs),\n",
    "    'std': np.std(cmu_probs),\n",
    "    'min': np.min(cmu_probs),\n",
    "    'max': np.max(cmu_probs),\n",
    "    'median': np.median(cmu_probs)\n",
    "}\n",
    "\n",
    "print(\"\\nüìà BOS Token Completions Statistics:\")\n",
    "print(f\"   Mean log probability: {bos_stats['mean']:8.4f}\")\n",
    "print(f\"   Standard deviation:   {bos_stats['std']:8.4f}\")\n",
    "print(f\"   Minimum:             {bos_stats['min']:8.4f}\")\n",
    "print(f\"   Maximum:             {bos_stats['max']:8.4f}\")\n",
    "print(f\"   Median:              {bos_stats['median']:8.4f}\")\n",
    "\n",
    "print(\"\\nüìà CMU Prompt Completions Statistics:\")\n",
    "print(f\"   Mean log probability: {cmu_stats['mean']:8.4f}\")\n",
    "print(f\"   Standard deviation:   {cmu_stats['std']:8.4f}\")\n",
    "print(f\"   Minimum:             {cmu_stats['min']:8.4f}\")\n",
    "print(f\"   Maximum:             {cmu_stats['max']:8.4f}\")\n",
    "print(f\"   Median:              {cmu_stats['median']:8.4f}\")\n",
    "\n",
    "print(f\"\\nüîç Comparison:\")\n",
    "print(f\"   Mean difference (CMU - BOS): {cmu_stats['mean'] - bos_stats['mean']:8.4f}\")\n",
    "print(f\"   Std difference (CMU - BOS):  {cmu_stats['std'] - bos_stats['std']:8.4f}\")\n",
    "\n",
    "print(f\"\\nüñ•Ô∏è  System Information:\")\n",
    "print(f\"   Device used: {generator.device}\")\n",
    "print(f\"   Model: {generator.model_name}\")\n",
    "print(f\"   Torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8371a3",
   "metadata": {},
   "source": [
    "## Visualization of Probability Distributions\n",
    "\n",
    "Let's create visualizations to better understand the probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45d29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create probability distribution plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Histogram comparison\n",
    "ax1.hist(bos_probs, bins=20, alpha=0.7, label='BOS Token', density=True)\n",
    "ax1.hist(cmu_probs, bins=20, alpha=0.7, label='CMU Prompt', density=True)\n",
    "ax1.set_xlabel('Log Probability')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Probability Distribution Comparison')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Box plot comparison\n",
    "data_to_plot = [bos_probs, cmu_probs]\n",
    "labels = ['BOS Token', 'CMU Prompt']\n",
    "ax2.boxplot(data_to_plot, labels=labels)\n",
    "ax2.set_ylabel('Log Probability')\n",
    "ax2.set_title('Probability Distribution Box Plot')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b72d6",
   "metadata": {},
   "source": [
    "## Key Observations and Insights\n",
    "\n",
    "Based on our experiments, we can observe several interesting patterns:\n",
    "\n",
    "1. **Probability Ranges**: The model assigns different probability ranges to unconditional vs conditional generation\n",
    "2. **Diversity**: Both conditions produce diverse completions, but with different characteristics\n",
    "3. **Context Effect**: The CMU prompt constrains the generation space, affecting the probability distribution\n",
    "4. **Model Behavior**: High-probability completions tend to be more generic, while low-probability ones are more specific or creative\n",
    "\n",
    "This analysis demonstrates how language models balance between probable (common) and improbable (creative) text generation,\n",
    "and how conditioning context affects this balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cfe789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
