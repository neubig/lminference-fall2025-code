{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6667790",
   "metadata": {},
   "source": [
    "I am assuming we have access two jsonls\n",
    "- Student Outptuts\n",
    "    - `index`: Unique identifier for each question.\n",
    "    - `output`: The model's response to the question.\n",
    "\n",
    "- Hidden Test Set with the following fields:\n",
    "    - `index`: Unique identifier for each question.\n",
    "    - `task`: The name of the task (e.g., \"mmlu_med\").\n",
    "    - `prompt`: The question prompt presented to the model.\n",
    "    - `gold_answer`: The correct answer to the question.\n",
    "    - (Not needed)`meta`: Additional metadata about the question, including unique id in the dataset and other fields.\n",
    "\n",
    "For this grading logic, we assume we can get the task and other info by essentially grouping by `index` from the hidden test set and joining with the student outputs on `index`.\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4231eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from grader import (\n",
    "    InfoBenchEvaluator,\n",
    "    GraphEvaluator,\n",
    "    MMLUEvaluator,\n",
    "    ResponseParser,\n",
    "    evaluate_single\n",
    ")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836a265",
   "metadata": {},
   "source": [
    "## create test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3802c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math_verify\n",
    "# test = math_verify.parse(\"\\\\boxed{B}\")\n",
    "# gt = \"B\"\n",
    "# math_verify.verify(gt, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c83db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created student_outputs.jsonl\n"
     ]
    }
   ],
   "source": [
    "student_outputs_data = [\n",
    "    # Index 1: Graph (1 path) - CORRECT (function call format)\n",
    "    {\n",
    "        \"index\": 1,\n",
    "        \"output\": \"To find the shortest path from node 0 to node 9, I'll use Dijkstra's algorithm.\\n\\nLooking at the edges from node 0: 0->8 has weight 3, which is the smallest.\\nFrom node 8: 8->9 has weight 22.\\nTotal: 3 + 22 = 25\\n\\nsubmit_paths(paths=[[0, 8, 9]], weights=[25])\"\n",
    "    },\n",
    "\n",
    "    # Index 2: Graph (3 paths) - PARTIALLY CORRECT (2 of 3, gold-like format)\n",
    "    {\n",
    "        \"index\": 2,\n",
    "        \"output\": \"Finding top 3 shortest paths from 0 to 15:\\n\\n1. 0 -> 7 -> 8 -> 15: 77 + 45 + 108 = 230\\n2. 0 -> 4 -> 8 -> 15: 125 + 28 + 108 = 261\\n\\n{\\\"paths\\\": [{\\\"path\\\": [0, 7, 8, 15], \\\"weight\\\": 230}, {\\\"path\\\": [0, 4, 8, 15], \\\"weight\\\": 261}]}\"\n",
    "    },\n",
    "\n",
    "    # Index 3: InfoBench (PyTorch NN) - GOOD (has comments, correct structure)\n",
    "    {\n",
    "        \"index\": 3,\n",
    "        \"output\": \"\"\"```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a two-hidden layer feedforward neural network\n",
    "class TwoHiddenLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TwoHiddenLayerNN, self).__init__()\n",
    "\n",
    "        # First hidden layer with 64 neurons\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "\n",
    "        # Second hidden layer with 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first hidden layer with ReLU\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        # Pass through second hidden layer with ReLU\n",
    "        x = self.relu(self.fc2(x))\n",
    "\n",
    "        # Output layer (no activation for raw logits)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = TwoHiddenLayerNN(input_size=10, output_size=2)\n",
    "```\"\"\"\n",
    "    },\n",
    "\n",
    "    # Index 4: InfoBench (Email) - PARTIAL (is email, about salary, but too short/informal)\n",
    "    {\n",
    "        \"index\": 4,\n",
    "        \"output\": \"Subject: Salary\\n\\nHi,\\n\\nI want more money.\\n\\nThanks\"\n",
    "    },\n",
    "\n",
    "    # Index 5: MMLU - CORRECT\n",
    "    {\n",
    "        \"index\": 5,\n",
    "        \"output\": \"Let me analyze each option:\\n\\n- Glucose: ~4 kcal/gram\\n- Palmitic acid (fat): ~9 kcal/gram\\n- Leucine (amino acid): ~4 kcal/gram\\n- Alcohol: ~7 kcal/gram\\n\\nFats release the most energy when oxidized. Palmitic acid is a fatty acid.\\n\\nThe answer is \\\\boxed{B}\"\n",
    "    },\n",
    "\n",
    "    # Index 6: MMLU - WRONG (chose A instead of C)\n",
    "    {\n",
    "        \"index\": 6,\n",
    "        \"output\": \"The patient has elevated lymphocytes, which suggests leukemia. Since they're B-cell origin, it's lymphocytic. The answer is \\\\boxed{A}\"\n",
    "    }\n",
    "]\n",
    "# Save to file\n",
    "with open(\"student_outputs.jsonl\", \"w\") as f:\n",
    "    for item in student_outputs_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "print(\"Created student_outputs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ccf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hidden_test(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load hidden test JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_student_outputs(path: str) -> Dict[int, str]:\n",
    "    \"\"\"Load student outputs JSONL, return dict mapping index -> output.\"\"\"\n",
    "    outputs = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                outputs[item[\"index\"]] = item.get(\"output\", \"\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781b93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(data: list, path: str):\n",
    "    \"\"\"Save list of dicts to JSONL.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "def save_json(data: dict, path: str):\n",
    "    \"\"\"Save dict to JSON.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24b5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTe: Remove later helper for printing metrics\n",
    "\n",
    "def print_metrics(metrics: dict):\n",
    "    \"\"\"Print metrics summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"RESULTS: {metrics['student_id']}\")\n",
    "    print(\"=\" * 50)\n",
    "    for task, m in metrics[\"task_metrics\"].items():\n",
    "        print(f\"{task:12s}: {m['accuracy']:.4f} ({m['count']} examples)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'OVERALL':12s}: {metrics['overall_accuracy']:.4f}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d2a06",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f708f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "def calculate_metrics(results: list, student_id: str) -> dict:\n",
    "    \"\"\"Calculate task-wise and overall metrics.\"\"\"\n",
    "    task_scores = {\"mmlu_med\": [], \"graph\": [], \"infobench\": []}\n",
    "\n",
    "    for r in results:\n",
    "        task = r[\"task\"]\n",
    "        if task in task_scores:\n",
    "            task_scores[task].append(r[\"score\"])\n",
    "\n",
    "    metrics = {\n",
    "        \"student_id\": student_id,\n",
    "        \"total_examples\": len(results),\n",
    "        \"task_metrics\": {},\n",
    "        \"overall_accuracy\": 0.0\n",
    "    }\n",
    "\n",
    "    all_scores = []\n",
    "    for task, scores in task_scores.items():\n",
    "        if scores:\n",
    "            metrics[\"task_metrics\"][task] = {\n",
    "                \"count\": len(scores),\n",
    "                \"accuracy\": sum(scores) / len(scores),\n",
    "                \"total_score\": sum(scores)\n",
    "            }\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "    if all_scores:\n",
    "        metrics[\"overall_accuracy\"] = sum(all_scores) / len(all_scores)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d2dac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(\n",
    "    hidden_test: list,\n",
    "    student_outputs: dict,\n",
    "    infobench_evaluator: InfoBenchEvaluator\n",
    ") -> list:\n",
    "    \"\"\"Run evaluation on all test items.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx, test_item in enumerate(tqdm(hidden_test, desc=\"Evaluating\")):\n",
    "        index = test_item[\"index\"]\n",
    "        student_response = student_outputs.get(index, \"\")\n",
    "        result = evaluate_single(idx, test_item, student_response, infobench_evaluator)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7924f4",
   "metadata": {},
   "source": [
    "# RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d37351",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e71859fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Hidden test size: 6\n",
      "Student outputs: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Configuration ===\n",
    "HIDDEN_TEST_PATH = \"combined_dataset.jsonl\"\n",
    "STUDENT_OUTPUT_PATH = \"student_outputs.jsonl\"\n",
    "OUTPUT_DIR = \"./eval_results\"\n",
    "STUDENT_ID = \"test_student\"\n",
    "EVAL_MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "if not openai_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "# === Load data ===\n",
    "print(\"Loading data...\")\n",
    "hidden_test = load_hidden_test(HIDDEN_TEST_PATH)\n",
    "student_outputs = load_student_outputs(STUDENT_OUTPUT_PATH)\n",
    "\n",
    "print(f\"Hidden test size: {len(hidden_test)}\")\n",
    "print(f\"Student outputs: {len(student_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be80fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing InfoBench evaluator...\n",
      "Verifying OpenAI connection...\n",
      "OpenAI connection verified ✓\n"
     ]
    }
   ],
   "source": [
    "# === Initialize InfoBench Evaluator ===\n",
    "print(\"\\nInitializing InfoBench evaluator...\")\n",
    "infobench_evaluator = InfoBenchEvaluator(openai_key, EVAL_MODEL)\n",
    "\n",
    "print(\"Verifying OpenAI connection...\")\n",
    "if not infobench_evaluator.verify_connection():\n",
    "    raise RuntimeError(\"OpenAI connection failed - cannot proceed\")\n",
    "print(\"OpenAI connection verified ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c98ca0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: test_student\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|█▋        | 1/6 [00:01<00:05,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous answer: I’d be happy to help you evaluate your email for salary negotiation. Please share the content of the email, and I can provide feedback on its suitability, tone, and effectiveness.\n",
      "Ambiguous answer: I can help you determine that. If you provide the content of the email, I can analyze it and let you know if it pertains to salary negotiation or not.\n",
      "Ambiguous answer: When I generate Python code, I can include comments that clarify each step of the process if explicitly asked to do so. Comments are a great way to improve code readability and help others (or your future self) understand what the code is doing. If you have a specific task or code you would like to see, please provide the details, and I can generate the code with explaining comments for you.\n",
      "Ambiguous answer: To determine if a generated Python code constructs a two-hidden layer feedforward neural network using the PyTorch `torch.nn` module, I would need to see the actual code you're referring to. That way, I can analyze it and confirm whether it meets the specified criteria.\n",
      "\n",
      "If you provide the code, I can help you review it for the following key aspects:\n",
      "\n",
      "1. **Use of `torch.nn.Module`:** The code should define a class that inherits from `torch.nn.Module`.\n",
      "\n",
      "2. **Layer Construction:** There should be two hidden layers defined using `torch.nn.Linear` or other layer types.\n",
      "\n",
      "3. **Activation Functions:** Typically, activation functions like ReLU or Sigmoid should be applied after the hidden layers.\n",
      "\n",
      "4. **Forward Method:** A `forward` method should be defined to specify how data flows through the network.\n",
      "\n",
      "Please share the code snippet, and I'll be happy to help you analyze it!\n",
      "Ambiguous answer: To determine if the architecture designed in your Python code consists of an input layer, two hidden layers, and an output layer, you would typically look for the following components in the code:\n",
      "\n",
      "1. **Input Layer**: This is where the data enters the neural network. In most frameworks (like TensorFlow or PyTorch), this might be indicated by the shape or size of the first layer.\n",
      "\n",
      "2. **Hidden Layers**: The presence of two hidden layers should be evident in the architecture definition. Hidden layers are typically represented by dense layers or similar constructs, depending on the framework.\n",
      "\n",
      "3. **Output Layer**: This layer produces the final output of the model. It is usually the last layer in the architecture and often has a size that corresponds to the number of classes in classification problems or a singular output for regression tasks.\n",
      "\n",
      "For example, in Keras, a simple neural network might be defined as follows:\n",
      "\n",
      "```python\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Dense\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Dense(units=64, activation='relu', input_shape=(input_dim,)))  # Input layer + first hidden layer\n",
      "model.add(Dense(units=64, activation='relu'))  # Second hidden layer\n",
      "model.add(Dense(units=1, activation='sigmoid'))  # Output layer\n",
      "```\n",
      "\n",
      "In this example:\n",
      "- The first layer defines the input size and serves as the first hidden layer.\n",
      "- The second layer is the second hidden layer.\n",
      "- The final layer is the output layer.\n",
      "\n",
      "If you'd like to provide the specific code snippet you have in mind, I can analyze it in detail to confirm if it matches your description of the architecture.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [00:08<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambiguous answer: To determine whether a Python code snippet ensures that the hidden layers of a neural network utilize the ReLU (Rectified Linear Unit) activation function, I would need to see the specific code you're referring to. \n",
      "\n",
      "However, I can guide you on how to check or implement it. In popular deep learning frameworks like TensorFlow or PyTorch, the ReLU activation is typically used in hidden layers like this:\n",
      "\n",
      "### In TensorFlow/Keras:\n",
      "\n",
      "```python\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Dense(64, activation='relu', input_shape=(input_size,)))  # First hidden layer with ReLU\n",
      "model.add(Dense(64, activation='relu'))  # Second hidden layer with ReLU\n",
      "model.add(Dense(1, activation='sigmoid'))  # Output layer (e.g., for binary classification)\n",
      "```\n",
      "\n",
      "### In PyTorch:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class MyModel(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(MyModel, self).__init__()\n",
      "        self.hidden1 = nn.Linear(input_size, 64)\n",
      "        self.hidden2 = nn.Linear(64, 64)\n",
      "        self.output = nn.Linear(64, 1)\n",
      "        \n",
      "    def forward(self, x):\n",
      "        x = nn.ReLU()(self.hidden1(x))  # ReLU activation after first hidden layer\n",
      "        x = nn.ReLU()(self.hidden2(x))  # ReLU activation after second hidden layer\n",
      "        x = torch.sigmoid(self.output(x))  # Output layer (e.g., for binary classification)\n",
      "        return x\n",
      "```\n",
      "\n",
      "In these examples, the hidden layers explicitly use the ReLU activation function. If your code follows a similar pattern where `activation='relu'` is provided or `nn.ReLU()` is called after the linear layers, then it confirms that ReLU is being utilized in the hidden layers.\n",
      "\n",
      "If you provide the actual code you're working with, I could analyze it and confirm whether it meets this criterion!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Run Evaluation ===\n",
    "print(f\"\\nEvaluating: {STUDENT_ID}\")\n",
    "print(\"-\" * 50)\n",
    "results = run_eval(hidden_test, student_outputs, infobench_evaluator)\n",
    "\n",
    "# === Calculate Metrics ===\n",
    "metrics = calculate_metrics(results, STUDENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4829b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS: test_student\n",
      "==================================================\n",
      "mmlu_med    : 0.5000 (2 examples)\n",
      "graph       : 0.8333 (2 examples)\n",
      "infobench   : 1.0000 (2 examples)\n",
      "--------------------------------------------------\n",
      "OVERALL     : 0.7778\n",
      "==================================================\n",
      "\n",
      "Results saved to: ./eval_results/test_student_results.jsonl\n",
      "Metrics saved to: ./eval_results/test_student_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# === Save Results ===\n",
    "results_path = os.path.join(OUTPUT_DIR, f\"{STUDENT_ID}_results.jsonl\")\n",
    "metrics_path = os.path.join(OUTPUT_DIR, f\"{STUDENT_ID}_metrics.json\")\n",
    "\n",
    "save_jsonl(results, results_path)\n",
    "save_json(metrics, metrics_path)\n",
    "\n",
    "# === Print Summary ===\n",
    "print_metrics(metrics)\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(f\"Metrics saved to: {metrics_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
