{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b8fb6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csna/mambaforge/envs/inf-algs/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json, os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0e8713",
   "metadata": {},
   "source": [
    "## Benchmarking with vs without KV caching\n",
    "\n",
    "1. Use [`meta-llama/Llama-3.1-8B`](https://huggingface.co/meta-llama/Llama-3.1-8B) and sweep `max_new_tokens`=`min_new_tokens`  by powers of two from 1 to 512, using the same short prompt (\"Once upon a time,\") for all output sequence lengths. Plot time vs output sequence length.\n",
    "2. Now repeat the same with `use_cache=False` in `model.generate()` -- add this plot to the same figure. Make sure to include a legend and descriptive labels/titles. Describe the trends you see in 1-2 sentences -- play around with both log scales and linear scales for a clearer idea of the trends -- **you will only need to include one version in your report**, however.\n",
    "3. Repeat 1-2 with a much longer prompt, read in from `long_prompt.txt`. **For this question, you only need to sweep from 1 to 32 without the KV cache -- with KV cache, you should be able to reach 512 without issues.** In ~2 sentences, (instead of just comparing KV cache vs no KV cache) compare the trends you see with those from your first plot, and provide an explanation for the trends you observe.\n",
    "4. Repeat 1-2 with [`meta-llama/Llama-3.2-1B`](https://huggingface.co/meta-llama/Llama-3.2-1B). Again, in ~2 sentences, compare the trends you see with those from your first plot, and provide an explanation for the trends you observe.\n",
    "5. Repeat 1-2 with [`meta-llama/Llama-2-7b`](https://huggingface.co/meta-llama/Llama-2-7b). Once again, write ~2 sentences to compare the trends you see with those from your first plot, and provide an explanation for the trends you observe.\n",
    "\n",
    "In total, you will include four (4) figures, one (1) explanation of the general kv vs no kv trend, and reflections for three (3) pairwise comparisons in your report. You may need to restart your kernel each time you want to use a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8194e2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_prompt = \"Once upon a time,\"\n",
    "\n",
    "with open(\"long_prompt.txt\") as f:\n",
    "    long_prompt = f.read().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c3742",
   "metadata": {},
   "source": [
    "### One generation example\n",
    "\n",
    "The code below should run without modification in an environment that contains the necessary libraries. A TA used 1 L40S GPU to run these exact cells.\n",
    "\n",
    "You can adapt it to complete the questions above. Again, **you may need to restart your kernel in between loading in different models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e1360c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 57.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-3.1-8B', dtype='bfloat16').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b87c929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of 256 new tokens with KV cache took 6 seconds.\n",
      "-----\n",
      "Output: ['<|begin_of_text|>Once upon a time, there was a man who worked as a bus driver. One day, he was driving his bus when he saw a young boy running along the side of the road. The boy was trying to catch up to the bus, but he was having a hard time keeping up. The bus driver felt sorry for the boy and decided to stop the bus so that he could help him get on board.\\nThe boy was very grateful and thanked the bus driver for his kindness. He told the bus driver that he was running late for school and that he had been trying to catch the bus for the past few minutes. The bus driver assured the boy that he would get him to school on time.\\nThe boy sat down in the seat next to the bus driver and they began to chat. The boy told the bus driver that his name was John and that he was in the third grade. The bus driver told John that his name was Mr. Smith and that he was a bus driver.\\nJohn asked Mr. Smith if he could ask him a question. Mr. Smith said that he was happy to answer any questions that John had. John asked Mr. Smith if he could ask him a question. Mr. Smith said that he was happy to answer any questions that John had. John asked Mr.']\n"
     ]
    }
   ],
   "source": [
    "prompt_toks = tokenizer(short_prompt, return_tensors=\"pt\").to('cuda')\n",
    "start = time.time()\n",
    "torch.cuda.synchronize()\n",
    "output = model.generate(**prompt_toks, max_new_tokens=256, use_cache=True) # true by default\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(\"Generation of\", output.shape[1]-prompt_toks['input_ids'].shape[1], \"new tokens with KV cache took\", round(end-start), \"seconds.\")\n",
    "print(\"-----\")\n",
    "print(\"Output:\", tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf893cce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84891ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation of 256 new tokens without KV cache took 8 seconds.\n",
      "-----\n",
      "Output: ['<|begin_of_text|>Once upon a time, I had a very different life. I was a professional, living in a big city, with a high-powered job, a beautiful house, a fancy car and a lot of money. But I was miserable. I was always stressed out, and I felt like I was running on a hamster wheel, going nowhere. I was so unhappy that I decided to quit my job and move to a small town in the middle of nowhere. I wanted to start over, to find happiness and fulfillment in a new place. And that’s when I discovered the magic of small town life.\\nSmall town life is something that many people dream of. It’s a place where everyone knows each other, where the pace of life is slower and more relaxed, and where people are more connected to the land and to each other. It’s a place where you can live a simpler, more authentic life, where you can focus on what’s important, and where you can find a sense of community and belonging.\\nSmall town life is not for everyone, of course. It’s not for people who are looking for a fast-paced, high-energy lifestyle. But for those who are looking for something different, something more meaningful, small town life can be a breath of fresh air. It can be a place']\n"
     ]
    }
   ],
   "source": [
    "prompt_toks = tokenizer(short_prompt, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "start = time.time()\n",
    "torch.cuda.synchronize()\n",
    "output = model.generate(**prompt_toks, max_new_tokens=256, use_cache=False) # no KV cache\n",
    "torch.cuda.synchronize()\n",
    "end = time.time()\n",
    "print(\"Generation of\", output.shape[1]-prompt_toks['input_ids'].shape[1], \"new tokens without KV cache took\", round(end-start), \"seconds.\")\n",
    "print(\"-----\")\n",
    "print(\"Output:\", tokenizer.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca085054",
   "metadata": {},
   "source": [
    "### Example plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037829f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "times_with_kv = []\n",
    "times_no_kv = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbeffa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(10), times_with_kv, label=\"with kv cache\") # or use [2**x for x in range(10)] for linear scale\n",
    "plt.plot(range(10), times_no_kv, label=\"no kv cache\")\n",
    "xticks = [f\"2^{x}\" for x in range(10)]\n",
    "plt.xticks(list(range(10)), labels=xticks)\n",
    "plt.legend()\n",
    "# TODO: titles, etc.\n",
    "plt.savefig(\"[YOUR FILEPATH HERE]\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf-algs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
