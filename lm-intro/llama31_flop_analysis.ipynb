{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34441851",
   "metadata": {},
   "source": [
    "# Llama 3.1 FLOP Analysis and Visualization\n",
    "\n",
    "This notebook analyzes the computational requirements (FLOPs) of different Llama 3.1 model variants\n",
    "and visualizes how these requirements scale with context length and model size.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll examine:\n",
    "- FLOP scaling with context length for different model sizes\n",
    "- Distribution of FLOPs across different components (attention, MLP, etc.)\n",
    "- Computational efficiency metrics\n",
    "- Architecture comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b23f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib for better readability\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 20,\n",
    "    'axes.labelsize': 16,\n",
    "    'xtick.labelsize': 13,\n",
    "    'ytick.labelsize': 13,\n",
    "    'legend.fontsize': 14,\n",
    "    'figure.figsize': (12, 8)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bbafb",
   "metadata": {},
   "source": [
    "## Model Configurations\n",
    "\n",
    "First, let's define the configurations for the three Llama 3.1 model variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c2f8fe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Llama 3.1 model configurations\n",
    "LLAMA31_CONFIGS = {\n",
    "    \"Llama-3.1-8B\": {\n",
    "        \"hidden_size\": 4096,\n",
    "        \"intermediate_size\": 14336,\n",
    "        \"num_hidden_layers\": 32,\n",
    "        \"num_attention_heads\": 32,\n",
    "        \"num_key_value_heads\": 8,\n",
    "        \"vocab_size\": 128256,\n",
    "        \"max_position_embeddings\": 131072,\n",
    "        \"head_dim\": 128,\n",
    "        \"parameters\": 8.0e9\n",
    "    },\n",
    "    \"Llama-3.1-70B\": {\n",
    "        \"hidden_size\": 8192,\n",
    "        \"intermediate_size\": 28672,\n",
    "        \"num_hidden_layers\": 80,\n",
    "        \"num_attention_heads\": 64,\n",
    "        \"num_key_value_heads\": 8,\n",
    "        \"vocab_size\": 128256,\n",
    "        \"max_position_embeddings\": 131072,\n",
    "        \"head_dim\": 128,\n",
    "        \"parameters\": 70.6e9\n",
    "    },\n",
    "    \"Llama-3.1-405B\": {\n",
    "        \"hidden_size\": 16384,\n",
    "        \"intermediate_size\": 53248,\n",
    "        \"num_hidden_layers\": 126,\n",
    "        \"num_attention_heads\": 128,\n",
    "        \"num_key_value_heads\": 8,\n",
    "        \"vocab_size\": 128256,\n",
    "        \"max_position_embeddings\": 131072,\n",
    "        \"head_dim\": 128,\n",
    "        \"parameters\": 405.9e9\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the configurations\n",
    "for model_name, config in LLAMA31_CONFIGS.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Parameters: {config['parameters']/1e9:.1f}B\")\n",
    "    print(f\"  Hidden size: {config['hidden_size']}\")\n",
    "    print(f\"  Layers: {config['num_hidden_layers']}\")\n",
    "    print(f\"  Attention heads: {config['num_attention_heads']}\")\n",
    "    print(f\"  KV heads: {config['num_key_value_heads']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef52991",
   "metadata": {},
   "source": [
    "## FLOP Calculation Function\n",
    "\n",
    "Now let's implement the function to calculate FLOPs for a forward pass through the model.\n",
    "This includes attention (both linear and quadratic components), MLP layers, and the language modeling head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b636c567",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_flops_per_token(config: Dict, sequence_length: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate FLOPs per forward pass for a given model configuration and sequence length.\n",
    "    \n",
    "    Args:\n",
    "        config: Model configuration dictionary\n",
    "        sequence_length: Input sequence length\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with FLOP breakdown by component\n",
    "    \"\"\"\n",
    "    h = config[\"hidden_size\"]\n",
    "    i = config[\"intermediate_size\"]\n",
    "    v = config[\"vocab_size\"]\n",
    "    n_layers = config[\"num_hidden_layers\"]\n",
    "    n_heads = config[\"num_attention_heads\"]\n",
    "    n_kv_heads = config[\"num_key_value_heads\"]\n",
    "    head_dim = config[\"head_dim\"]\n",
    "    seq_len = sequence_length\n",
    "    \n",
    "    # === ATTENTION LAYER FLOPS ===\n",
    "    # Q, K, V projections (linear in sequence length)\n",
    "    q_proj_flops = 2 * seq_len * h * (n_heads * head_dim)\n",
    "    k_proj_flops = 2 * seq_len * h * (n_kv_heads * head_dim)\n",
    "    v_proj_flops = 2 * seq_len * h * (n_kv_heads * head_dim)\n",
    "    qkv_proj_flops = q_proj_flops + k_proj_flops + v_proj_flops\n",
    "    \n",
    "    # Q @ K^T (quadratic in sequence length)\n",
    "    qk_flops = 2 * n_heads * seq_len * seq_len * head_dim\n",
    "    \n",
    "    # Attention weights @ V (quadratic in sequence length)\n",
    "    attn_v_flops = 2 * n_heads * seq_len * seq_len * head_dim\n",
    "    \n",
    "    # Output projection\n",
    "    o_proj_flops = 2 * seq_len * (n_heads * head_dim) * h\n",
    "    \n",
    "    attention_flops_per_layer = qkv_proj_flops + qk_flops + attn_v_flops + o_proj_flops\n",
    "    \n",
    "    # === MLP LAYER FLOPS ===\n",
    "    # SwiGLU MLP: gate, up, down projections\n",
    "    gate_proj_flops = 2 * seq_len * h * i\n",
    "    up_proj_flops = 2 * seq_len * h * i\n",
    "    down_proj_flops = 2 * seq_len * i * h\n",
    "    mlp_flops_per_layer = gate_proj_flops + up_proj_flops + down_proj_flops\n",
    "    \n",
    "    # Layer norm FLOPs (minimal)\n",
    "    layernorm_flops_per_layer = 2 * seq_len * h\n",
    "    \n",
    "    # Total per layer\n",
    "    flops_per_layer = attention_flops_per_layer + mlp_flops_per_layer + layernorm_flops_per_layer\n",
    "    \n",
    "    # Final layer norm and language modeling head\n",
    "    final_layernorm_flops = seq_len * h\n",
    "    lm_head_flops = 2 * seq_len * h * v\n",
    "    \n",
    "    # Total FLOPs\n",
    "    total_flops = (n_layers * flops_per_layer) + final_layernorm_flops + lm_head_flops\n",
    "    \n",
    "    # Separate linear and quadratic attention components for analysis\n",
    "    attention_linear = qkv_proj_flops + o_proj_flops\n",
    "    attention_quadratic = qk_flops + attn_v_flops\n",
    "    \n",
    "    return {\n",
    "        \"attention_linear\": attention_linear,\n",
    "        \"attention_quadratic\": attention_quadratic,\n",
    "        \"attention_per_layer\": attention_flops_per_layer,\n",
    "        \"mlp_per_layer\": mlp_flops_per_layer,\n",
    "        \"layernorm_per_layer\": layernorm_flops_per_layer,\n",
    "        \"flops_per_layer\": flops_per_layer,\n",
    "        \"total_layers_flops\": n_layers * flops_per_layer,\n",
    "        \"final_layernorm\": final_layernorm_flops,\n",
    "        \"lm_head\": lm_head_flops,\n",
    "        \"total\": total_flops\n",
    "    }\n",
    "\n",
    "# Test the function with a simple example\n",
    "test_flops = calculate_flops_per_token(LLAMA31_CONFIGS[\"Llama-3.1-8B\"], 2048)\n",
    "print(f\"Example: Llama-3.1-8B with 2K context requires {test_flops['total']/1e12:.2f} TFLOPs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0cd6e",
   "metadata": {},
   "source": [
    "## FLOP Scaling Analysis\n",
    "\n",
    "Let's analyze how FLOPs scale with context length for different model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de38fcb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_flop_scaling_figure():\n",
    "    \"\"\"Create figure showing FLOP scaling with context length.\"\"\"\n",
    "    context_lengths = [1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    fig.subplots_adjust(bottom=0.28, top=0.90)\n",
    "    \n",
    "    # Plot 1: Total FLOPs vs Context Length\n",
    "    for model_name, config in LLAMA31_CONFIGS.items():\n",
    "        flops_data = []\n",
    "        for ctx_len in context_lengths:\n",
    "            flops = calculate_flops_per_token(config, ctx_len)\n",
    "            flops_data.append(flops['total'] / 1e12)  # Convert to TFLOPs\n",
    "        \n",
    "        ax1.scatter(context_lengths, flops_data, label=model_name, s=80, alpha=0.75)\n",
    "    \n",
    "    ax1.set_xlabel('Context Length (tokens)', fontsize=16)\n",
    "    ax1.set_ylabel('Total FLOPs (TFLOPs)', fontsize=16)\n",
    "    ax1.set_title('Llama 3.1 FLOP Scaling with Context Length', fontsize=20, fontweight='bold')\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='lower center', bbox_to_anchor=(0.5, -0.30), ncol=3,\n",
    "               fontsize=14, frameon=True, framealpha=0.95)\n",
    "    \n",
    "    # Add context length labels\n",
    "    ax1.set_xticks(context_lengths)\n",
    "    ax1.set_xticklabels([f'{x//1024}K' if x >= 1024 else str(x) for x in context_lengths])\n",
    "    \n",
    "    # Plot 2: FLOP Distribution for 70B model\n",
    "    model_70b = LLAMA31_CONFIGS[\"Llama-3.1-70B\"]\n",
    "    context_lengths_breakdown = [1024, 4096, 16384, 65536, 131072]\n",
    "    \n",
    "    attention_linear_ratios = []\n",
    "    attention_quadratic_ratios = []\n",
    "    mlp_ratios = []\n",
    "    lm_head_ratios = []\n",
    "    \n",
    "    print(\"\\n=== FLOP Distribution Analysis: Llama-3.1-70B ===\")\n",
    "    \n",
    "    for ctx_len in context_lengths_breakdown:\n",
    "        flops = calculate_flops_per_token(model_70b, ctx_len)\n",
    "        n_layers = model_70b[\"num_hidden_layers\"]\n",
    "        \n",
    "        att_linear = flops['attention_linear'] * n_layers\n",
    "        att_quad = flops['attention_quadratic'] * n_layers\n",
    "        mlp = flops['mlp_per_layer'] * n_layers\n",
    "        lm_head = flops['lm_head']\n",
    "        total = att_linear + att_quad + mlp + lm_head\n",
    "        \n",
    "        # Calculate ratios (percentages)\n",
    "        att_linear_ratio = (att_linear / total) * 100\n",
    "        att_quad_ratio = (att_quad / total) * 100\n",
    "        mlp_ratio = (mlp / total) * 100\n",
    "        lm_head_ratio = (lm_head / total) * 100\n",
    "        \n",
    "        attention_linear_ratios.append(att_linear_ratio)\n",
    "        attention_quadratic_ratios.append(att_quad_ratio)\n",
    "        mlp_ratios.append(mlp_ratio)\n",
    "        lm_head_ratios.append(lm_head_ratio)\n",
    "        \n",
    "        print(f\"Context {ctx_len:>6}: Att(Lin)={att_linear_ratio:>5.1f}%, \"\n",
    "              f\"Att(Quad)={att_quad_ratio:>5.1f}%, MLP={mlp_ratio:>5.1f}%, LM={lm_head_ratio:>4.1f}%\")\n",
    "    \n",
    "    x = np.arange(len(context_lengths_breakdown))\n",
    "    width = 0.6\n",
    "    \n",
    "    p1 = ax2.bar(x, attention_linear_ratios, width, label='Attention (Linear)', alpha=0.8)\n",
    "    p2 = ax2.bar(x, attention_quadratic_ratios, width, bottom=attention_linear_ratios, \n",
    "                label='Attention (Quadratic)', alpha=0.8)\n",
    "    p3 = ax2.bar(x, mlp_ratios, width, \n",
    "                bottom=np.array(attention_linear_ratios) + np.array(attention_quadratic_ratios), \n",
    "                label='MLP Layers', alpha=0.8)\n",
    "    p4 = ax2.bar(x, lm_head_ratios, width, \n",
    "                bottom=np.array(attention_linear_ratios) + np.array(attention_quadratic_ratios) + np.array(mlp_ratios), \n",
    "                label='LM Head', alpha=0.8)\n",
    "    \n",
    "    ax2.set_xlabel('Context Length', fontsize=16)\n",
    "    ax2.set_ylabel('FLOP Percentage (%)', fontsize=16)\n",
    "    ax2.set_title('Llama-3.1-70B FLOP Distribution by Context Length', fontsize=20, fontweight='bold')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([f'{ctx//1024}K' for ctx in context_lengths_breakdown])\n",
    "    ax2.legend(loc='lower center', bbox_to_anchor=(0.5, -0.28), ncol=2,\n",
    "               fontsize=14, frameon=True, framealpha=0.95)\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the scaling analysis\n",
    "create_flop_scaling_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf78c29",
   "metadata": {},
   "source": [
    "## Computational Efficiency Analysis\n",
    "\n",
    "Let's examine the computational efficiency in terms of FLOPs per parameter and memory vs compute trade-offs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b292f12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_efficiency_comparison():\n",
    "    \"\"\"Create figure comparing computational efficiency metrics.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: FLOPs per Parameter vs Model Size\n",
    "    models = []\n",
    "    params = []\n",
    "    flops_per_param_8k = []\n",
    "    flops_per_param_32k = []\n",
    "    flops_per_param_131k = []\n",
    "    \n",
    "    for model_name, config in LLAMA31_CONFIGS.items():\n",
    "        models.append(model_name.replace('Llama-3.1-', ''))\n",
    "        params.append(config['parameters'] / 1e9)  # Convert to billions\n",
    "        \n",
    "        # Calculate FLOPs per parameter for different context lengths\n",
    "        flops_8k = calculate_flops_per_token(config, 8192)\n",
    "        flops_32k = calculate_flops_per_token(config, 32768)\n",
    "        flops_131k = calculate_flops_per_token(config, 131072)\n",
    "        \n",
    "        flops_per_param_8k.append(flops_8k['total'] / config['parameters'])\n",
    "        flops_per_param_32k.append(flops_32k['total'] / config['parameters'])\n",
    "        flops_per_param_131k.append(flops_131k['total'] / config['parameters'])\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax1.bar(x - width, flops_per_param_8k, width, label='8K Context', alpha=0.8)\n",
    "    ax1.bar(x, flops_per_param_32k, width, label='32K Context', alpha=0.8)\n",
    "    ax1.bar(x + width, flops_per_param_131k, width, label='131K Context', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Model', fontsize=12)\n",
    "    ax1.set_ylabel('FLOPs per Parameter', fontsize=12)\n",
    "    ax1.set_title('Computational Efficiency: FLOPs per Parameter', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(models)\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Memory vs Compute Trade-off\n",
    "    context_lengths = [8192, 32768, 131072]\n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for i, ctx_len in enumerate(context_lengths):\n",
    "        memory_usage = []  # KV cache memory (simplified)\n",
    "        compute_flops = []\n",
    "        \n",
    "        for model_name, config in LLAMA31_CONFIGS.items():\n",
    "            # Simplified KV cache memory calculation (assuming fp16)\n",
    "            kv_cache_size = (2 * config['num_key_value_heads'] * config['head_dim'] * \n",
    "                           config['num_hidden_layers'] * ctx_len * 2) / 1e9  # GB\n",
    "            memory_usage.append(kv_cache_size)\n",
    "            \n",
    "            flops = calculate_flops_per_token(config, ctx_len)\n",
    "            compute_flops.append(flops['total'] / 1e12)  # TFLOPs\n",
    "        \n",
    "        ax2.scatter(memory_usage, compute_flops, s=100, alpha=0.7, \n",
    "                   label=f'{ctx_len//1024}K Context', color=colors[i])\n",
    "        \n",
    "        # Add model labels\n",
    "        for j, model in enumerate(models):\n",
    "            ax2.annotate(model, (memory_usage[j], compute_flops[j]), \n",
    "                        xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax2.set_xlabel('KV Cache Memory (GB)', fontsize=12)\n",
    "    ax2.set_ylabel('Compute FLOPs (TFLOPs)', fontsize=12)\n",
    "    ax2.set_title('Memory vs Compute Trade-off', fontsize=14, fontweight='bold')\n",
    "    ax2.legend(fontsize=11)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the efficiency comparison\n",
    "create_efficiency_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72d8024",
   "metadata": {},
   "source": [
    "## Architecture Comparison\n",
    "\n",
    "Finally, let's compare the architectural components across different model sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf937ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_architecture_comparison():\n",
    "    \"\"\"Create figure comparing architectural components.\"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    models = list(LLAMA31_CONFIGS.keys())\n",
    "    model_labels = [name.replace('Llama-3.1-', '') for name in models]\n",
    "    \n",
    "    # Plot 1: Parameter Distribution\n",
    "    embedding_params = []\n",
    "    attention_params = []\n",
    "    mlp_params = []\n",
    "    \n",
    "    for model_name, config in LLAMA31_CONFIGS.items():\n",
    "        h = config[\"hidden_size\"]\n",
    "        i = config[\"intermediate_size\"]\n",
    "        v = config[\"vocab_size\"]\n",
    "        n_layers = config[\"num_hidden_layers\"]\n",
    "        n_heads = config[\"num_attention_heads\"]\n",
    "        n_kv_heads = config[\"num_key_value_heads\"]\n",
    "        head_dim = config[\"head_dim\"]\n",
    "        \n",
    "        # Calculate parameters (simplified)\n",
    "        emb = v * h / 1e9\n",
    "        att_per_layer = (h * (n_heads * head_dim + 2 * n_kv_heads * head_dim) + \n",
    "                        (n_heads * head_dim) * h) * n_layers / 1e9\n",
    "        mlp_per_layer = (h * i * 3) * n_layers / 1e9  # gate, up, down projections\n",
    "        \n",
    "        embedding_params.append(emb)\n",
    "        attention_params.append(att_per_layer)\n",
    "        mlp_params.append(mlp_per_layer)\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.6\n",
    "    \n",
    "    p1 = ax1.bar(x, embedding_params, width, label='Embedding', alpha=0.8)\n",
    "    p2 = ax1.bar(x, attention_params, width, bottom=embedding_params, label='Attention', alpha=0.8)\n",
    "    p3 = ax1.bar(x, mlp_params, width, \n",
    "                bottom=np.array(embedding_params) + np.array(attention_params), \n",
    "                label='MLP', alpha=0.8)\n",
    "    \n",
    "    ax1.set_xlabel('Model', fontsize=12)\n",
    "    ax1.set_ylabel('Parameters (Billions)', fontsize=12)\n",
    "    ax1.set_title('Parameter Distribution by Component', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(model_labels)\n",
    "    ax1.legend(fontsize=11)\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Layer Scaling\n",
    "    layers = [config[\"num_hidden_layers\"] for config in LLAMA31_CONFIGS.values()]\n",
    "    hidden_sizes = [config[\"hidden_size\"] for config in LLAMA31_CONFIGS.values()]\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(models)))\n",
    "    scatter = ax2.scatter(layers, hidden_sizes, s=200, c=colors, alpha=0.7)\n",
    "    \n",
    "    for i, model in enumerate(model_labels):\n",
    "        ax2.annotate(model, (layers[i], hidden_sizes[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    ax2.set_xlabel('Number of Layers', fontsize=12)\n",
    "    ax2.set_ylabel('Hidden Size', fontsize=12)\n",
    "    ax2.set_title('Architecture Scaling: Layers vs Hidden Size', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Attention Head Configuration\n",
    "    attention_heads = [config[\"num_attention_heads\"] for config in LLAMA31_CONFIGS.values()]\n",
    "    kv_heads = [config[\"num_key_value_heads\"] for config in LLAMA31_CONFIGS.values()]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, attention_heads, width, label='Query/Key/Value Heads', alpha=0.8)\n",
    "    ax3.bar(x + width/2, kv_heads, width, label='Key/Value Heads (GQA)', alpha=0.8)\n",
    "    \n",
    "    ax3.set_xlabel('Model', fontsize=12)\n",
    "    ax3.set_ylabel('Number of Heads', fontsize=12)\n",
    "    ax3.set_title('Attention Head Configuration', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(model_labels)\n",
    "    ax3.legend(fontsize=11)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 4: MLP Expansion Ratio\n",
    "    expansion_ratios = []\n",
    "    for config in LLAMA31_CONFIGS.values():\n",
    "        ratio = config[\"intermediate_size\"] / config[\"hidden_size\"]\n",
    "        expansion_ratios.append(ratio)\n",
    "    \n",
    "    bars = ax4.bar(model_labels, expansion_ratios, alpha=0.8, color=colors)\n",
    "    ax4.set_xlabel('Model', fontsize=12)\n",
    "    ax4.set_ylabel('MLP Expansion Ratio', fontsize=12)\n",
    "    ax4.set_title('MLP Expansion Ratio (Intermediate/Hidden)', fontsize=14, fontweight='bold')\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, ratio in zip(bars, expansion_ratios):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                f'{ratio:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the architecture comparison\n",
    "create_architecture_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f73ea59",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis reveals several key insights about Llama 3.1 models:\n",
    "\n",
    "1. **Quadratic Scaling**: Attention computation scales quadratically with context length, becoming dominant at very long contexts\n",
    "2. **Model Efficiency**: Larger models are more parameter-efficient in terms of FLOPs per parameter\n",
    "3. **Memory Trade-offs**: KV cache memory grows linearly with context length and can become a bottleneck\n",
    "4. **Architecture Patterns**: All models use the same expansion ratio (3.5x) and GQA configuration (8 KV heads)\n",
    "\n",
    "These insights are crucial for understanding the computational requirements and trade-offs when deploying Llama 3.1 models in production environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c8a69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
