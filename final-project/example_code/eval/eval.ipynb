{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6667790",
   "metadata": {},
   "source": [
    "I am assuming we have access two jsonls\n",
    "- Student Outptuts\n",
    "    - `index`: Unique identifier for each question.\n",
    "    - `output`: The model's response to the question.\n",
    "\n",
    "- Hidden Test Set with the following fields:\n",
    "    - `index`: Unique identifier for each question.\n",
    "    - `task`: The name of the task (e.g., \"mmlu_med\").\n",
    "    - `prompt`: The question prompt presented to the model.\n",
    "    - `gold_answer`: The correct answer to the question.\n",
    "    - (Not needed)`meta`: Additional metadata about the question, including unique id in the dataset and other fields.\n",
    "\n",
    "For this grading logic, we assume we can get the task and other info by essentially grouping by `index` from the hidden test set and joining with the student outputs on `index`.\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4231eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Any\n",
    "from grader import (\n",
    "    InfoBenchEvaluator,\n",
    "    GraphEvaluator,\n",
    "    MMLUEvaluator,\n",
    "    ResponseParser,\n",
    "    evaluate_single\n",
    ")\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836a265",
   "metadata": {},
   "source": [
    "## create test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3802c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import math_verify\n",
    "# test = math_verify.parse(\"\\\\boxed{B}\")\n",
    "# # response = \"The answer is (C).\"\n",
    "# response = \"The answer is B.\"\n",
    "# gt = \"B\"\n",
    "\n",
    "# def extract_answer(response: str) -> str:\n",
    "#     answer_match = re.search(r'The answer is\\s*\\(?([A-Z])\\)?', response, re.IGNORECASE)\n",
    "#     if answer_match:\n",
    "#         return [answer_match.group(1).upper()]\n",
    "#     return []\n",
    "\n",
    "# answer = extract_answer(response)\n",
    "# print(answer)\n",
    "# math_verify.verify(gt, test), math_verify.verify(gt, answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c83db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created student_outputs.jsonl\n"
     ]
    }
   ],
   "source": [
    "student_outputs_data = [\n",
    "    # Index 1: Graph (1 path) - CORRECT (function call format)\n",
    "    {\n",
    "        \"index\": 1,\n",
    "        \"output\": \"To find the shortest path from node 0 to node 9, I'll use Dijkstra's algorithm.\\n\\nLooking at the edges from node 0: 0->8 has weight 3, which is the smallest.\\nFrom node 8: 8->9 has weight 22.\\nTotal: 3 + 22 = 25\\n\\nsubmit_paths(paths=[[0, 8, 9]], weights=[25])\"\n",
    "    },\n",
    "\n",
    "    # Index 2: Graph (3 paths) - PARTIALLY CORRECT (2 of 3, gold-like format)\n",
    "    {\n",
    "        \"index\": 2,\n",
    "        \"output\": \"Finding top 3 shortest paths from 0 to 15:\\n\\n1. 0 -> 7 -> 8 -> 15: 77 + 45 + 108 = 230\\n2. 0 -> 4 -> 8 -> 15: 125 + 28 + 108 = 261\\n\\n{\\\"paths\\\": [{\\\"path\\\": [0, 7, 8, 15], \\\"weight\\\": 230}, {\\\"path\\\": [0, 4, 8, 15], \\\"weight\\\": 261}]}\"\n",
    "    },\n",
    "\n",
    "    # Index 3: InfoBench (PyTorch NN) - GOOD (has comments, correct structure)\n",
    "    {\n",
    "        \"index\": 3,\n",
    "        \"output\": \"\"\"```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a two-hidden layer feedforward neural network\n",
    "class TwoHiddenLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TwoHiddenLayerNN, self).__init__()\n",
    "\n",
    "        # First hidden layer with 64 neurons\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "\n",
    "        # Second hidden layer with 64 neurons\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through first hidden layer with ReLU\n",
    "        x = self.relu(self.fc1(x))\n",
    "\n",
    "        # Pass through second hidden layer with ReLU\n",
    "        x = self.relu(self.fc2(x))\n",
    "\n",
    "        # Output layer (no activation for raw logits)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "model = TwoHiddenLayerNN(input_size=10, output_size=2)\n",
    "```\"\"\"\n",
    "    },\n",
    "\n",
    "    # Index 4: InfoBench (Email) - PARTIAL (is email, about salary, but too short/informal)\n",
    "    {\n",
    "        \"index\": 4,\n",
    "        \"output\": \"Subject: Salary\\n\\nHi,\\n\\nI want more money.\\n\\nThanks\"\n",
    "    },\n",
    "\n",
    "    # Index 5: MMLU - CORRECT\n",
    "    {\n",
    "        \"index\": 5,\n",
    "        \"output\": \"Let me analyze each option:\\n\\n- Glucose: ~4 kcal/gram\\n- Palmitic acid (fat): ~9 kcal/gram\\n- Leucine (amino acid): ~4 kcal/gram\\n- Alcohol: ~7 kcal/gram\\n\\nFats release the most energy when oxidized. Palmitic acid is a fatty acid.\\n\\nThe answer is \\\\boxed{B}\"\n",
    "    },\n",
    "\n",
    "    # Index 6: MMLU - WRONG (chose A instead of C)\n",
    "    {\n",
    "        \"index\": 6,\n",
    "        \"output\": \"The patient has elevated lymphocytes, which suggests leukemia. Since they're B-cell origin, it's lymphocytic. The answer is \\\\boxed{A}\"\n",
    "    }\n",
    "]\n",
    "# Save to file\n",
    "with open(\"student_outputs.jsonl\", \"w\") as f:\n",
    "    for item in student_outputs_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "print(\"Created student_outputs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1ccf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hidden_test(path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load hidden test JSONL file.\"\"\"\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_student_outputs(path: str) -> Dict[int, str]:\n",
    "    \"\"\"Load student outputs JSONL, return dict mapping index -> output.\"\"\"\n",
    "    outputs = {}\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                outputs[item[\"index\"]] = item.get(\"output\", \"\")\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "781b93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_jsonl(data: list, path: str):\n",
    "    \"\"\"Save list of dicts to JSONL.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "\n",
    "def save_json(data: dict, path: str):\n",
    "    \"\"\"Save dict to JSON.\"\"\"\n",
    "    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c24b5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTe: Remove later helper for printing metrics\n",
    "\n",
    "def print_metrics(metrics: dict):\n",
    "    \"\"\"Print metrics summary.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"RESULTS: {metrics['student_id']}\")\n",
    "    print(\"=\" * 50)\n",
    "    for task, m in metrics[\"task_metrics\"].items():\n",
    "        print(f\"{task:12s}: {m['accuracy']:.4f} ({m['count']} examples)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'OVERALL':12s}: {metrics['overall_accuracy']:.4f}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d2a06",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f708f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "def calculate_metrics(results: list, student_id: str) -> dict:\n",
    "    \"\"\"Calculate task-wise and overall metrics.\"\"\"\n",
    "    task_scores = {\"mmlu_med\": [], \"graph\": [], \"infobench\": []}\n",
    "\n",
    "    for r in results:\n",
    "        task = r[\"task\"]\n",
    "        if task in task_scores:\n",
    "            task_scores[task].append(r[\"score\"])\n",
    "\n",
    "    metrics = {\n",
    "        \"student_id\": student_id,\n",
    "        \"total_examples\": len(results),\n",
    "        \"task_metrics\": {},\n",
    "        \"overall_accuracy\": 0.0\n",
    "    }\n",
    "\n",
    "    all_scores = []\n",
    "    for task, scores in task_scores.items():\n",
    "        if scores:\n",
    "            metrics[\"task_metrics\"][task] = {\n",
    "                \"count\": len(scores),\n",
    "                \"accuracy\": sum(scores) / len(scores),\n",
    "                \"total_score\": sum(scores)\n",
    "            }\n",
    "            all_scores.extend(scores)\n",
    "\n",
    "    if all_scores:\n",
    "        metrics[\"overall_accuracy\"] = sum(all_scores) / len(all_scores)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d2dac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eval(\n",
    "    hidden_test: list,\n",
    "    student_outputs: dict,\n",
    "    infobench_evaluator: InfoBenchEvaluator\n",
    ") -> list:\n",
    "    \"\"\"Run evaluation on all test items.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for idx, test_item in enumerate(tqdm(hidden_test, desc=\"Evaluating\")):\n",
    "        index = test_item[\"index\"]\n",
    "        student_response = student_outputs.get(index, \"\")\n",
    "        result = evaluate_single(idx, test_item, student_response, infobench_evaluator)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7924f4",
   "metadata": {},
   "source": [
    "# RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d37351",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e71859fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Hidden test size: 6\n",
      "Student outputs: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Configuration ===\n",
    "HIDDEN_TEST_PATH = \"combined_dataset.jsonl\"\n",
    "STUDENT_OUTPUT_PATH = \"student_outputs.jsonl\"\n",
    "OUTPUT_DIR = \"./eval_results\"\n",
    "STUDENT_ID = \"test_student\"\n",
    "EVAL_MODEL = \"gpt-5-nano-2025-08-07\"\n",
    "\n",
    "if not openai_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "# === Load data ===\n",
    "print(\"Loading data...\")\n",
    "hidden_test = load_hidden_test(HIDDEN_TEST_PATH)\n",
    "student_outputs = load_student_outputs(STUDENT_OUTPUT_PATH)\n",
    "\n",
    "print(f\"Hidden test size: {len(hidden_test)}\")\n",
    "print(f\"Student outputs: {len(student_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be80fc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing InfoBench evaluator...\n"
     ]
    }
   ],
   "source": [
    "# # === Initialize InfoBench Evaluator ===\n",
    "print(\"\\nInitializing InfoBench evaluator...\")\n",
    "infobench_evaluator = InfoBenchEvaluator(openai_key, EVAL_MODEL)\n",
    "\n",
    "# print(\"Verifying OpenAI connection...\")\n",
    "# if not infobench_evaluator.verify_connection():\n",
    "#     raise RuntimeError(\"OpenAI connection failed - cannot proceed\")\n",
    "# print(\"OpenAI connection verified ✓\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c98ca0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating: test_student\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Is the generated text an email? => YES\n",
      "Q2: Is the generated email about a salary negotiation? => YES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  17%|█▋        | 1/6 [00:14<01:14, 14.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q3: Is the generated email suitable for salary negotiation? => NO\n",
      "Q1: Is the generated text a Python code snippet? => YES\n",
      "Q2: Does the generated Python code construct a two-hidden layer feedforward neural network using the PyTorch `torch.nn` module? => YES\n",
      "Q3: Does the architecture designed in the Python code consist of an input layer, two hidden layers, and an output layer? => NO\n",
      "Q4: Does the Python code ensure that the hidden layers of the network utilize the ReLU activation function? => YES\n",
      "Q5: Does the neuron count in each hidden layer of the network, as implemented in the Python code, range between 32 and 128 to maintain a reasonably sized network? => YES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 6/6 [00:34<00:00,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q6: Does the generated Python code include comments that clarify each step of the process? => YES\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Run Evaluation ===\n",
    "print(f\"\\nEvaluating: {STUDENT_ID}\")\n",
    "print(\"-\" * 50)\n",
    "results = run_eval(hidden_test, student_outputs, infobench_evaluator)\n",
    "\n",
    "# === Calculate Metrics ===\\\n",
    "metrics = calculate_metrics(results, STUDENT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4829b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS: test_student\n",
      "==================================================\n",
      "mmlu_med    : 0.5000 (2 examples)\n",
      "graph       : 0.8333 (2 examples)\n",
      "infobench   : 0.7500 (2 examples)\n",
      "--------------------------------------------------\n",
      "OVERALL     : 0.6944\n",
      "==================================================\n",
      "\n",
      "Results saved to: ./eval_results/test_student_results.jsonl\n",
      "Metrics saved to: ./eval_results/test_student_metrics.json\n"
     ]
    }
   ],
   "source": [
    "# === Save Results ===\n",
    "results_path = os.path.join(OUTPUT_DIR, f\"{STUDENT_ID}_results.jsonl\")\n",
    "metrics_path = os.path.join(OUTPUT_DIR, f\"{STUDENT_ID}_metrics.json\")\n",
    "\n",
    "save_jsonl(results, results_path)\n",
    "save_json(metrics, metrics_path)\n",
    "\n",
    "# === Print Summary ===\n",
    "print_metrics(metrics)\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(f\"Metrics saved to: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e72c467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839a1322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
